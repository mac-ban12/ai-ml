{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"flowers_oxford.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":["MhpsQYo5Dwny","SPgfyGzTDwoQ","--NtUVNiDwo4"]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"bu5nLaAqDwm9","colab_type":"text"},"source":["# Flowers classification - Oxford flowers 102 dataset \n"]},{"cell_type":"markdown","metadata":{"id":"Ddc_AiPzDwm_","colab_type":"text"},"source":["We have created a 102 category dataset, consisting of 102 flower categories. The flowers chosen to be flower commonly occuring in the United Kingdom. Each class consists of between 40 and 258 images. The details of the categories and the number of images for each class can be found on this category statistics page.\n","\n","The images have large scale, pose and light variations. In addition, there are categories that have large variations within the category and several very similar categories. The dataset is visualized using isomap with shape and colour features."]},{"cell_type":"code","metadata":{"id":"kWqktHCTDwnA","colab_type":"code","colab":{}},"source":["%reload_ext autoreload\n","%autoreload 2\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ghQEOLU6E2U7","colab_type":"code","colab":{}},"source":["# Set up fastai for collab \n","!curl -s https://course.fast.ai/setup/colab | bash\n","!pip uninstall torch torchvision -y\n","!pip install torch==1.4.0 torchvision==0.5.0"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iCrIUc2uqq6g","colab_type":"text"},"source":["Set up drive access for file storage "]},{"cell_type":"code","metadata":{"id":"CNBF-A3OFian","colab_type":"code","colab":{}},"source":["#from google.colab import drive\n","#drive.mount('/content/gdrive', force_remount=True)\n","#root_dir = \"/content/gdrive/My Drive/\"\n","#base_dir = root_dir + 'fastai-v3/'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZksTSYkWDwnF","colab_type":"text"},"source":["Import fastai libs"]},{"cell_type":"code","metadata":{"id":"Oh_MrFu-DwnG","colab_type":"code","colab":{}},"source":["from fastai.vision import *\n","from fastai.metrics import error_rate"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mSYaXMj0DwnL","colab_type":"text"},"source":["If you're using a computer with an unusually small GPU, you may get an out of memory error when running this notebook. If this happens, click Kernel->Restart, uncomment the 2nd line below to use a smaller *batch size* (you'll learn all about what this means during the course), and try again."]},{"cell_type":"code","metadata":{"id":"6TLFzy-XDwnM","colab_type":"code","colab":{}},"source":["bs = 64\n","# bs = 16   # uncomment this line if you run out of memory even after clicking Kernel->Restart"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9CBgqHgsDwnP","colab_type":"text"},"source":["## Looking at the data / Data preparation\n"]},{"cell_type":"markdown","metadata":{"id":"C7QGrPjVDwnQ","colab_type":"text"},"source":["https://www.robots.ox.ac.uk/~vgg/data/flowers/102/\n","\n","We are going to use the `untar_data` function to which we must pass a URL as an argument and which will download and extract the data."]},{"cell_type":"code","metadata":{"id":"9EcmT8J0DwnW","colab_type":"code","colab":{}},"source":["path = untar_data(URLs.FLOWERS); path"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w25CITv6Dwna","colab_type":"code","colab":{}},"source":["path.ls()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_awFxzVtDwne","colab_type":"code","colab":{}},"source":["#path_anno = path/'annotations'\n","path_img = path/'jpg'\n","path_train = path/'train.txt'\n","path_valid = path/'valid.txt'\n","path_test = path/'test.txt'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BxyHvWXqDwnh","colab_type":"text"},"source":["The first thing we do when we approach a problem is to take a look at the data. We _always_ need to understand very well what the problem is and what the data looks like before we can figure out how to solve it. Taking a look at the data means understanding how the data directories are structured, what the labels are and what some sample images look like.\n","\n","Labels are stored in txt files - **train.txt** and **valid.txt** "]},{"cell_type":"code","metadata":{"id":"6ljZ6fDWDwni","colab_type":"code","colab":{}},"source":["fnames = get_image_files(path_img)\n","fnames[:5]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X1rV_iC5ydKp","colab_type":"code","colab":{}},"source":["#f = open (path_train, \"r\")\n","#print(\"Train data ---\")\n","#print(f.readline());\n","#print(f.readline())\n","\n","#print(\"Valid data ---\")\n","#f = open (path_valid, \"r\")\n","#print(f.readline())\n","#print(f.readline())\n","\n","#print(\"Test data ---\")\n","#f = open (path_test, \"r\")\n","#print(f.readline())\n","#print(f.readline())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x5-DVejR1PC5","colab_type":"text"},"source":["Create data frames for test and valid. Merge into single (train_df) as ImagedataBunch allocated 20% validation set. "]},{"cell_type":"code","metadata":{"id":"xEvQpLUMDwnl","colab_type":"code","colab":{}},"source":["import pandas as pd\n","\n","train_df = pd.read_fwf(path_train, header=None)\n","valid_df = pd.read_fwf(path_valid, header=None)\n","train_df = pd.concat ([train_df, valid_df])\n","train_df.sort_index"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"acR8_Q6wDwno","colab_type":"code","colab":{}},"source":["data = ImageDataBunch.from_df(path, train_df, ds_tfms=get_transforms(), size=224, bs=bs\n","                                  ).normalize(imagenet_stats)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qBvbzwMuDwnr","colab_type":"code","colab":{}},"source":["data.show_batch(rows=3, figsize=(7,6))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TKkPu68jDwnu","colab_type":"code","colab":{}},"source":["print(data.classes)\n","len(data.classes),data.c"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MhpsQYo5Dwny","colab_type":"text"},"source":["## Training: resnet34"]},{"cell_type":"markdown","metadata":{"id":"lPZjjOJXDwny","colab_type":"text"},"source":["Now we will start training our model. We will use a [convolutional neural network](http://cs231n.github.io/convolutional-networks/) backbone and a fully connected head with a single hidden layer as a classifier. Don't know what these things mean? Not to worry, we will dive deeper in the coming lessons. For the moment you need to know that we are building a model which will take images as input and will output the predicted probability for each of the categories (in this case, it will have 37 outputs).\n","\n","We will train for 4 epochs (4 cycles through all our data)."]},{"cell_type":"code","metadata":{"id":"VOX-ZAM5Dwnz","colab_type":"code","colab":{}},"source":["learn = cnn_learner(data, models.resnet34, metrics=error_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BlM2yhAJDwn2","colab_type":"code","colab":{}},"source":["learn.model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P_Qm3nhJDwoA","colab_type":"code","colab":{}},"source":["learn.fit_one_cycle(8)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g_FZGCEcDwoH","colab_type":"code","colab":{}},"source":["learn.save('stage-1')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SPgfyGzTDwoQ","colab_type":"text"},"source":["## Results"]},{"cell_type":"markdown","metadata":{"id":"5cEA3N8LDwoR","colab_type":"text"},"source":["Let's see what results we have got. \n","\n","We will first see which were the categories that the model most confused with one another. We will try to see if what the model predicted was reasonable or not. In this case the mistakes look reasonable (none of the mistakes seems obviously naive). This is an indicator that our classifier is working correctly. \n","\n","Furthermore, when we plot the confusion matrix, we can see that the distribution is heavily skewed: the model makes the same mistakes over and over again but it rarely confuses other categories. This suggests that it just finds it difficult to distinguish some specific categories between each other; this is normal behaviour."]},{"cell_type":"code","metadata":{"id":"8StWyEp5DwoS","colab_type":"code","colab":{}},"source":["interp = ClassificationInterpretation.from_learner(learn)\n","\n","losses,idxs = interp.top_losses()\n","\n","len(data.valid_ds)==len(losses)==len(idxs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7FI7w1j2Dwoa","colab_type":"code","colab":{}},"source":["interp.plot_top_losses(9, figsize=(15,11))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2-N6Yb-1Dwoh","colab_type":"code","colab":{}},"source":["#doc(interp.plot_top_losses)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jwN8QAgLDwoo","colab_type":"code","colab":{}},"source":["interp.plot_confusion_matrix(figsize=(12,12), dpi=60)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_rR09RDXDwow","colab_type":"code","colab":{}},"source":["interp.most_confused(min_val=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"--NtUVNiDwo4","colab_type":"text"},"source":["## Unfreezing, fine-tuning, and learning rates"]},{"cell_type":"markdown","metadata":{"id":"jd0jL0FmDwo7","colab_type":"text"},"source":["Since our model is working as we expect it to, we will *unfreeze* our model and train some more."]},{"cell_type":"code","metadata":{"id":"8lCqqyKmDwo8","colab_type":"code","colab":{}},"source":["learn.unfreeze()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qxZVxdacDwpD","colab_type":"code","colab":{}},"source":["learn.fit_one_cycle(1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qV40RQgVDwpN","colab_type":"code","colab":{}},"source":["learn.load('stage-1');"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zeVVIxf0DwpU","colab_type":"code","colab":{}},"source":["learn.lr_find()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZG48Y0ibDwpc","colab_type":"code","colab":{}},"source":["learn.recorder.plot()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uPrSLRgxDwpj","colab_type":"code","colab":{}},"source":["learn.unfreeze()\n","learn.fit_one_cycle(4, max_lr=slice(1e-6,1e-4))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Jjno2_zDwpq","colab_type":"text"},"source":["That's a pretty accurate model!"]},{"cell_type":"markdown","metadata":{"id":"tLjtmUKFDwpt","colab_type":"text"},"source":["## Training: resnet50 / Densenet121"]},{"cell_type":"markdown","metadata":{"id":"hgqh2B6dDwpv","colab_type":"text"},"source":["Now we will train in the same way as before but with one caveat: instead of using resnet34 as our backbone we will use resnet50 (resnet34 is a 34 layer residual network while resnet50 has 50 layers. It will be explained later in the course and you can learn the details in the [resnet paper](https://arxiv.org/pdf/1512.03385.pdf)).\n","\n","Basically, resnet50 usually performs better because it is a deeper network with more parameters. Let's see if we can achieve a higher performance here. To help it along, let's us use larger images too, since that way the network can see more detail. We reduce the batch size a bit since otherwise this larger network will require more GPU memory."]},{"cell_type":"code","metadata":{"id":"-wpQ1BVODwpw","colab_type":"code","colab":{}},"source":["#data = ImageDataBunch.from_df(path, train_df, ds_tfms=get_transforms(), size=224, bs=bs//2\n","#                                  ).normalize(imagenet_stats)\n","data = ImageDataBunch.from_df(path, train_df, ds_tfms=get_transforms(), size=224, bs=bs//2\n","                                  ).normalize(imagenet_stats)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JopxSIZuDwp3","colab_type":"code","colab":{}},"source":["learn = cnn_learner(data, models.densenet121, metrics=error_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MKmWR2ieDwp_","colab_type":"code","colab":{}},"source":["learn.lr_find()\n","learn.recorder.plot()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z3Gj6ITzDwqH","colab_type":"code","colab":{}},"source":["learn.fit_one_cycle(8)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t7-5Gq2JDwqp","colab_type":"code","colab":{}},"source":["learn.save('stage-1-50')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YEdzf2U9Dwqu","colab_type":"text"},"source":["Let's see if full fine-tuning helps:"]},{"cell_type":"code","metadata":{"id":"EptnN9MJDwqw","colab_type":"code","colab":{}},"source":["learn.unfreeze()\n","learn.fit_one_cycle(3, max_lr=slice(1e-4,1e-1))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iOwVEWwZDwq0","colab_type":"text"},"source":["If it doesn't, you can always go back to your previous model."]},{"cell_type":"code","metadata":{"id":"km4ddSEEDwq1","colab_type":"code","colab":{}},"source":["learn.load('stage-1-50');"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lVz0GlXGDwq6","colab_type":"code","colab":{}},"source":["interp = ClassificationInterpretation.from_learner(learn)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-1WHT6LXDwq-","colab_type":"code","colab":{}},"source":["interp.most_confused(min_val=2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HvwBQFrRPJk0","colab_type":"text"},"source":["Notes:\n","1. With ResNet50 - error rate ~14%. Full fine tuning with all layers degrades performance. Why? \n","2. With DenseNet121 - error rate ~ 8% - very good. Full fine tuning with all layers degrades performance. Why?\n","3. Check this - https://medium.com/@iamvince/flowers-classification-using-fastai-and-attaining-an-accuracy-above-95-28fd53b59940 "]}]}